<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>神经网络期中作业-表情识别</title>
    <link href="/2024/05/21/ANN-midterm-homework/"/>
    <url>/2024/05/21/ANN-midterm-homework/</url>
    
    <content type="html"><![CDATA[<p>太久没更博客了，这次先更一个期中作业的报告。本次实验要求实现一个表情识别模型，对五种表情进行分类，并计算分类的准确性。在本次实验中，自己通过数据增强、网络结构优化和模型融合，**将模型在测试集上的的识别准确率由开始的57%提高到72%**。</p><p><img src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517164431436.png" alt="image-20240517164431436"></p><h3 id="一-数据预处理"><a href="#一-数据预处理" class="headerlink" title="一. 数据预处理"></a>一. 数据预处理</h3><p>本次实验的数据集储存在一个文件夹中，每个文件夹下属5个子文件夹，名字为各个类别的label（Angry，Happy等）。这种格式适合于pytroch中的<code>datasets.ImageFolder</code>，这个函数可以将文件夹的所有图片载入进来，同时各个子文件夹的名字作为图片的label：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">train_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>)<br>test_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/test&#x27;</span>, transform=transform)<br></code></pre></td></tr></table></figure><p>对于每张图片，我们需要将其从图片格式转换为张量，对于每张图片，我们将其转换为一个48×48的矩阵，然后将其变为tensor，并对其中的值进行归一化，在归一化中，我们采用(0.485, 0.456, 0.406)和(0.229, 0.224, 0.225)作为归一化的均值和方差，这个值是从ImageNet数据集上通过大量图片数据分析得出的结果，是目前CV领域的一个常见归一化：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python">transform = transforms.Compose([<br>    transforms.Resize((<span class="hljs-number">48</span>, <span class="hljs-number">48</span>)),  <span class="hljs-comment"># 将图像调整为48x48大小</span><br>    transforms.ToTensor(),<br>    transforms.Normalize(mean=[<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>], std=[<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])  <span class="hljs-comment"># 采用ImageNet的均值和标准差</span><br>])<br></code></pre></td></tr></table></figure><p>然而，此次训练集只有几千张图片，并且各个图片数量并不均匀，为了提高模型的表现，对于训练集采用特别的变换，具体来说，这里增加了<strong>随机裁剪，随机反转，随机旋转</strong>三种数据增强方法。通过数据增强，模型能够提升泛化性，然而数据增强也可能导致模型学习到一些噪声或难以收敛，本次作业测量了5种数据增强方法对于模型性能的提高效果，具体数据见</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">train_transform = transforms.Compose([<br>    transforms.RandomResizedCrop(<span class="hljs-number">48</span>, scale=(<span class="hljs-number">0.8</span>, <span class="hljs-number">1.2</span>)), <span class="hljs-comment">#随机裁剪</span><br>    transforms.RandomHorizontalFlip(), <span class="hljs-comment">#随机翻转</span><br>    transforms.RandomApply([transforms.RandomRotation(<span class="hljs-number">10</span>)], p=<span class="hljs-number">0.5</span>), <span class="hljs-comment">#随机旋转10°，有50%概率发生</span><br>    transforms.ToTensor(),<br>    transforms.Normalize((<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>), (<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>)),<br>]) <br></code></pre></td></tr></table></figure><p>接下来需要构建验证集，从训练集取出20%的图片作为验证集，需要注意的是，只有训练集需要应用<code>train_transform</code>，而验证集不需要进行数据增强。因此，这里先不用任何变换随机分割数据集，然后将对应的变换放给训练集和验证集，最后替换原先随机分割的样本：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python">num_train = <span class="hljs-built_in">len</span>(train_dataset) <br>num_val = <span class="hljs-built_in">int</span>(<span class="hljs-number">0.2</span> * num_train) <span class="hljs-comment">#计算验证集的大小</span><br>num_train_actual = num_train - num_val<br><span class="hljs-comment">#随机分割数据集（此时不应用任何变换）</span><br>train_data, val_data = random_split(train_dataset, [num_train_actual, num_val])<br><span class="hljs-comment">#应用不同的变换到训练集和验证集</span><br>train_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>, transform=train_transform)<br>val_dataset = datasets.ImageFolder(root=<span class="hljs-string">&#x27;mid_hw_emotion_recognition/train&#x27;</span>, transform=transform)<br><span class="hljs-comment">#替换原来随机分割的数据集中的样本</span><br>train_dataset.samples = train_data.dataset.samples<br>val_dataset.samples = val_data.dataset.samples<br></code></pre></td></tr></table></figure><h3 id="二-神经网络结构"><a href="#二-神经网络结构" class="headerlink" title="二. 神经网络结构"></a>二. 神经网络结构</h3><h4 id="（一）CNN"><a href="#（一）CNN" class="headerlink" title="（一）CNN"></a>（一）CNN</h4><p>普通的CNN在理论课已经有比较详细的讲解，在这里开始构建了一个两层的CNN网络，初始的<code>in_channel</code>是3（因为图是彩色的），每个卷积层有3个卷积核，因为步长为1，为了保持图像维度不变进行1个padding。池化层采用最大池化。因为经过了两层池化，所以最后每张图片大小是64×12×12（48&#x2F;2&#x2F;2），再经过两层全连接层后，得到分类的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">CNN</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(CNN, self).__init__()<br>        self.conv1 = nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.conv2 = nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        self.pool = nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)<br>        self.fc1 = nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">12</span> * <span class="hljs-number">12</span>, <span class="hljs-number">128</span>)<br>        self.fc2 = nn.Linear(<span class="hljs-number">128</span>, <span class="hljs-number">5</span>)  <br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        x = self.pool(torch.relu(self.conv1(x)))<br>        x = self.pool(torch.relu(self.conv2(x)))<br>        x = torch.flatten(x, <span class="hljs-number">1</span>)<br>        x = torch.relu(self.fc1(x))<br>        x = self.fc2(x)<br>        <span class="hljs-keyword">return</span> x<br></code></pre></td></tr></table></figure><h4 id="（二）Resnet"><a href="#（二）Resnet" class="headerlink" title="（二）Resnet"></a>（二）Resnet</h4><p>对于传统CNN，其层数不断增加后，可能会出现反向传播梯度消失的风险，因此传统CNN层数受到一定的限制，Resnet采用残差连接解决了这个问题，从而可以将层数不断增加。本次作业根据pytorch的Resnet18模型，实现了模型的神经网络结构。对于Resnet，其结构如下所示，图像在经过一层卷积层和池化后进入残差连接层。每个残差连接层有两层卷积层，每个卷积层有64个3×3的卷积核。</p><p><img src="https://pic2.zhimg.com/v2-97e5b4c6d1142e1dffa938215374dac9_b.jpg" alt="img"></p><p>在每个残差连接层中，其构造如下所示，相较于CNN，Resnet也采用了Batch Normalization，这也是为了防止数据过拟合。同时，为了保证输入输出的维度保持一致，在计算残差时需要进行一次判定，如果维度不符，那么就需要通过卷积层再做一次提取，这个操作主要见于两个大残差层连接的时候，因为前一层<code>out_channel</code>是第二个大残差层<code>out_channel</code>的1&#x2F;2，因此需要做一次卷积将其放大维度。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResBlock</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channel, out_channel, stride=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>(ResBlock, self).__init__()<br>        self.conv1 = torch.nn.Conv2d(in_channel, out_channel, kernel_size=<span class="hljs-number">3</span>, stride=stride, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = torch.nn.BatchNorm2d(out_channel)<br>        self.conv2 = torch.nn.Conv2d(out_channel, out_channel, kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn2 = torch.nn.BatchNorm2d(out_channel)<br>        <br>        self.shortcut = torch.nn.Sequential() <span class="hljs-comment"># 如果正常，shortcut就是直接为空，直接+x</span><br>        <span class="hljs-keyword">if</span> stride != <span class="hljs-number">1</span> <span class="hljs-keyword">or</span> in_channel != out_channel:<br>            self.shortcut = torch.nn.Sequential(<br>                torch.nn.Conv2d(in_channel, out_channel, kernel_size=<span class="hljs-number">1</span>, stride=stride, bias=<span class="hljs-literal">False</span>),<br>                torch.nn.BatchNorm2d(out_channel)<br>            )<br>        <span class="hljs-comment"># 这个if是为了防止维度不一致，如果维度不一致需要通过卷积层进行一次提取，以统一维度</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        y = F.relu(self.bn1(self.conv1(x)))<br>        y = self.bn2(self.conv2(y))<br>        y += self.shortcut(x) <span class="hljs-comment"># y除了进行变换，还加上了shortcut(x)</span><br>        <span class="hljs-keyword">return</span> F.relu(y)<br></code></pre></td></tr></table></figure><p>接下来，可以构造Resnet的网络架构（这里省略了forward），在该网络中，图像先经过一层卷积层和池化层，然后对于残差层，通过<code>_make_layer</code>来构造残差层。具体来说，这里的大残差层包括四层（<code>layer1</code>到<code>layer4</code>）。每个大残差层有着不同的输出通道，内部可以构造指定层数的小残差层。这样构造的好处在于可以通过改变<code>num_blocks</code>参数来改变网络的架构（例如从Resnet18到Resnet34）。再经过4个大残差层后，就可以通过avgpool进行一次池化，最后输出至全连接层，得到分类的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">ResNet</span>(torch.nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, block, num_blocks, num_classes=<span class="hljs-number">5</span></span>):<br>        <span class="hljs-built_in">super</span>(ResNet, self).__init__()<br>        self.in_channels = <span class="hljs-number">64</span><br>        self.conv1 = torch.nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>, bias=<span class="hljs-literal">False</span>)<br>        self.bn1 = torch.nn.BatchNorm2d(<span class="hljs-number">64</span>)<br>        self.relu = torch.nn.ReLU(inplace=<span class="hljs-literal">True</span>)<br>        self.maxpool = torch.nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.layer1 = self._make_layer(block, <span class="hljs-number">64</span>, num_blocks[<span class="hljs-number">0</span>], stride=<span class="hljs-number">1</span>)<br>        self.layer2 = self._make_layer(block, <span class="hljs-number">128</span>, num_blocks[<span class="hljs-number">1</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer3 = self._make_layer(block, <span class="hljs-number">256</span>, num_blocks[<span class="hljs-number">2</span>], stride=<span class="hljs-number">2</span>)<br>        self.layer4 = self._make_layer(block, <span class="hljs-number">512</span>, num_blocks[<span class="hljs-number">3</span>], stride=<span class="hljs-number">2</span>)<br>        self.avgpool = torch.nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))<br>        self.fc = torch.nn.Linear(<span class="hljs-number">512</span>, num_classes)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layer</span>(<span class="hljs-params">self, block, out_channels, num_blocks, stride</span>):<br>        <span class="hljs-comment"># 构建指定层数的残差层</span><br>        strides = [stride] + [<span class="hljs-number">1</span>]*(num_blocks-<span class="hljs-number">1</span>)<br>        layers = []<br>        <span class="hljs-keyword">for</span> stride <span class="hljs-keyword">in</span> strides:<br>            layers.append(block(self.in_channels, out_channels, stride))<br>            self.in_channels = out_channels<br>        <span class="hljs-keyword">return</span> torch.nn.Sequential(*layers)<br><br>model = ResNet(ResBlock, [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>], num_classes=<span class="hljs-number">5</span>).to(device) <span class="hljs-comment"># 残差层2*4*2+卷积层*1+全连接层*1=18</span><br></code></pre></td></tr></table></figure><h4 id="（三）VGG"><a href="#（三）VGG" class="headerlink" title="（三）VGG"></a>（三）VGG</h4><p>VGG实际上早于Resnet出现，相较于传统CNN，其通过重复使用简单的基础块来构建深度模型，具体来说，<strong>其就是一个大号的CNN</strong>。VGG可以分成几个相似的大层相连接，每个大层包括<strong>一个卷积层、一个BatchNorm层、以及一个激活函数</strong>。对于VGG11网络，其最大维度为512，并在大层中穿插池化层。</p><p>由于这几个大层相似，所以同样也可以根据上述的<code>_make_layer</code>进行构造，这里参考了[2]的实现：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">class</span> <span class="hljs-title class_">VGG</span>(nn.Module):<br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self</span>):<br>        <span class="hljs-built_in">super</span>(VGG, self).__init__()<br>        cfg=[<span class="hljs-number">64</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">128</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>, <span class="hljs-number">512</span>, <span class="hljs-number">512</span>, <span class="hljs-string">&#x27;M&#x27;</span>]<br>        self.features = self._make_layers(cfg)<br>        self.classifier = nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">5</span>)<br>        <br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">_make_layers</span>(<span class="hljs-params">self, cfg</span>):<br>        layers = []<br>        in_channels = <span class="hljs-number">3</span><br>        <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> cfg:<br>            <span class="hljs-keyword">if</span> x == <span class="hljs-string">&#x27;M&#x27;</span>:<br>                layers += [nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)]<br>            <span class="hljs-keyword">else</span>:<br>                layers += [nn.Conv2d(in_channels, x, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                           nn.BatchNorm2d(x),<br>                           nn.ReLU(inplace=<span class="hljs-literal">True</span>)]<br>                in_channels = x<br>        layers += [nn.AvgPool2d(kernel_size=<span class="hljs-number">1</span>, stride=<span class="hljs-number">1</span>)]<br>        <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure><h3 id="三-超参数设置"><a href="#三-超参数设置" class="headerlink" title="三. 超参数设置"></a>三. 超参数设置</h3><p>本次实验训练涉及以下超参数或优化器设置，在本次实验中，自己选择的设置为：</p><ul><li>学习率：0.001，这是因为在尝试多个学习率后，发现0.001能够较快的收敛，同时模型表现的性能较好，因此选取该学习率。</li><li>损失函数：交叉熵损失，分类常用损失函数，这里有了解Focal loss的使用方法，但其并不符合本次实验的数据情况，故依旧选取简单有效的交叉熵损失。</li><li>优化器：Adam，从经验来看Adam一般结果都不会太差，因此选取该优化器作为此次实验的优化器。</li><li>batch_size:64</li><li>num_epoch:100</li></ul><p>Q：是否要选取合适的学习率衰减方法？</p><p>A：本次实验尝试了ReduceLROnPlateau这一学习率衰减方法，其根据验证集上的loss更改学习率：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs python">scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=<span class="hljs-string">&#x27;min&#x27;</span>, factor=<span class="hljs-number">0.75</span>, patience=<span class="hljs-number">4</span>, verbose=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>然而结果显示，这两种学习率衰减方法应用后训练表现并不佳，个人认为这是由于0.001的学习率已经处于比较合适的范围，学习率继续衰减容易导致网络训练时参数更新过慢。并且在一些神经网络的训练上（例如VGG），其训练的时候验证集loss波动较大，因此如果采用ReduceLROnPlateau，很容易导致学习率进行一些不必要的衰减。所以最终自己并没有采用任何学习率衰减方法。为了验证，自己还在Resnet上进行了测试：</p><table><thead><tr><th>准确率\方法</th><th>None</th><th>ReduceLROnPlateau</th></tr></thead><tbody><tr><td>准确率</td><td>64.2%</td><td>63.6%</td></tr></tbody></table><h3 id="四-验证结果"><a href="#四-验证结果" class="headerlink" title="四. 验证结果"></a>四. 验证结果</h3><p>对于CNN，其训练图像如下所示，可以看到，训练100个epochs后测试集上准确率为59%。可以看到，传统CNN的效果并不是特别好，训练最后验证集loss训到0.2左右也训不下去了，因此，必须要优化网络结构，采用更为先进的结构才能提高模型的表现。</p><p><img src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212415719.png" alt="image-20240517212415719"></p><p>接下来，Resnet18的训练结果如下所示，相较于传统CNN，Resnet18最终在测试集的准确率为64.2%，相较于CNN提高了5.2%，并且最后验证集loss也训到了0.1以下：</p><p><img src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212626056.png" alt="image-20240517212626056"></p><p>而VGG11的结果是最好的，相较于Resnet18和CNN，训练了100个epochs最后在测试集上准确率为70.8%！相较于Resnet同样也提升了4.8%：</p><p><img src="C:\Users\15006\AppData\Roaming\Typora\typora-user-images\image-20240517212718720.png" alt="image-20240517212718720"></p><p>总的来看，从训练结果可以发现，<strong>Happy和Surprise这两个类别普遍较高</strong>，Angry在VGG11的表现上较好，达到了77%的准确度，而<strong>Neutral和Sad的准确率在各个模型上表现均不如其他类别</strong>。个人认为这是因为数据集中不同label的图像具有不同的区分度，对于Happy和Surprise，其表情幅度变化比较大，比较容易区分。但Neutral和Sad这两个类别的图像在一些情况下较难区分，从而降低了准确率。</p><h3 id="五-一些探索和心得体会"><a href="#五-一些探索和心得体会" class="headerlink" title="五. 一些探索和心得体会"></a>五. 一些探索和心得体会</h3><h4 id="（1）数据增强"><a href="#（1）数据增强" class="headerlink" title="（1）数据增强"></a>（1）数据增强</h4><p>在本次实验中，自己选用了随机裁剪，随机反转，随机旋转三种数据增强手段。数据增强可以对现有的数据集进行扩充，并让模型从多角度去分析图像。然而，不合适的数据增强可能引入误导的噪声，并且让模型难以训练。因此，本次实验自己尝试了五类数据增强方法，分别为随机裁剪，随机反转，随机旋转，随机颜色抖动，随机仿射变换。为了验证他们的有效性，自己采用Resnet在该数据集上训练了40个epoch，分别采用不同的数据增强方法，结果如下所示：</p><table><thead><tr><th>方法</th><th>Resnet</th><th>+随机裁剪</th><th>+随机反转</th><th>+随机旋转</th><th>+随机颜色抖动</th><th>+随机仿射变换</th></tr></thead><tbody><tr><td>准确率</td><td>61.2%</td><td>63.6%</td><td>64.2%</td><td>65.4%</td><td>59.6%</td><td>57.6%</td></tr></tbody></table><p>可以发现，随机颜色抖动和随机仿射变换使用后反而差于不使用的结果，而随机旋转对于模型性能提高最大，在测试集上甚至达到了65%的准确率。然而，随机颜色抖动和随机仿射变换的结果反而效果要差于不使用数据增强的结果。因此，自己选取随机反转、随机旋转、随机裁剪三种数据增强方法，作为此次训练集的transform。</p><h4 id="（2）模型融合"><a href="#（2）模型融合" class="headerlink" title="（2）模型融合"></a>（2）模型融合</h4><p>在训练完Resnet和VGG11模型后，自己思考是否能利用已有模型提升模型在测试集的性能。因此，模型融合成为自己尝试的方向。尽管Resnet在各类label预测准确度都不如VGG，然而，可能在一些图片上Resnet能够更好捕捉数据的特征，从而进行正确的分类。因此，自己对两个模型的输出进行平均，从而得到新的输出结果。由于VGG的表现要好于Resnet，因此自己采用加权平均对两个模型的输出进行融合。具体来说，VGG具有更高的权重（0.8），Resnet的权重则比较低（0.2），在进行加权平均后，得到新的输出。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">model_ensemble</span>(<span class="hljs-params">model1, model2, <span class="hljs-built_in">input</span>, weight1=<span class="hljs-number">0.8</span>, weight2=<span class="hljs-number">0.2</span></span>):<br>    model1.<span class="hljs-built_in">eval</span>()<br>    model2.<span class="hljs-built_in">eval</span>()<br>    <br>    <span class="hljs-keyword">with</span> torch.no_grad():<br>        output1 = model1(<span class="hljs-built_in">input</span>)<br>        output2 = model2(<span class="hljs-built_in">input</span>)<br>        <span class="hljs-comment"># 计算加权平均输出</span><br>        output = output1 * weight1 + output2 * weight2<br>    <br>    <span class="hljs-keyword">return</span> output<br></code></pre></td></tr></table></figure><p>结果显示，模型融合在测试集上展示了更为优越的成果：在模型融合上，模型输出准确率达到了71.8%.这一结果充分说明了模型融合的效果。</p><h4 id="（3）模型选择"><a href="#（3）模型选择" class="headerlink" title="（3）模型选择"></a>（3）模型选择</h4><p>在本次实验中，传统CNN只达到了59%的准确率，而Resnet和VGG11分别达到了64%和71%的准确率，这主要是因为传统CNN层数太浅，而Resnet18和VGG11通过残差连接和构建更深的网络达到了更好的效果。因此，在构建模型的时候，可以选择一些更为先进的网络结构，从而提高模型的表现效果。</p><h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>[1] <a href="https://zhuanlan.zhihu.com/p/451347771">https://zhuanlan.zhihu.com/p/451347771</a></p><p>[2] <a href="https://zhuanlan.zhihu.com/p/41423739">https://zhuanlan.zhihu.com/p/41423739</a></p><p>[3] <a href="https://zhuanlan.zhihu.com/p/33101420">https://zhuanlan.zhihu.com/p/33101420</a></p><p>[4] ChatGPT</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>复盘UIST 2024 &amp; 总结HCI一年的旅程</title>
    <link href="/2024/04/09/HCI-2024-04-09/"/>
    <url>/2024/04/09/HCI-2024-04-09/</url>
    
    <content type="html"><![CDATA[<p>北京时间4月6日凌晨00:20，我登上PCS再次查看自己的提交，abstract, paper, alter_figure, video整齐地排列在列表中，这也意味着半年的UIST项目到达了一个milestone的阶段。我也可以声称一下自己是“有一篇A会一作再投”的HCIer了233。但是提交之后，自己倒也没有很多的满足和高兴，而更多是“结束了”的喘息和“或许能做的更好”的遗憾。这段时间过后，自己也会将重心转移到考研上，所以我觉得也有必要在这里复盘一下这段工作的经历，并总结这一年HCI的学习。</p><p>说到这篇paper的idea，我觉得很难说是某个时候突然想出来的想法，在最开始自己体验AI角色扮演的APP时（glow），就体会到如果让用户单独发送信息，这样长期交互下用户必然导致疲惫和无聊，因此，在那时“让AI主动对话”就进入了我的视线；之后，自己遇到了一些挫折和失败，周围好友和亲人的主动关心也让我感到感动，所以我就在思考，<strong>当LLM向人们主动关心时，会带来怎么样的效果？</strong>于是，我就做了一个简单的PPT，希望做一个能<strong>主动关心（proactive support）的Chatbot</strong>。我和老师还有hz，yh讨论了一下，最后大家觉得这个是个有意义的话题，于是我们借助大创申请了实验的经费，开始做了起来。</p><p>2023年11月-2024年1月，我们开始了早期的工作，主要是去做idea iteration。具体的工作包括文献检索、用户访谈以及对话场景的讨论，当然，大创的开题答辩也是我们重点去做的一个工作，毕竟这也关系到我们的实验的经费。从这段时间来看，我们工作主要开展方式为：</p><ul><li><p>文献检索：主要检索了CSCW，CHI和UIST和Chatbot相关的论文（2021~2023），除了Chatbot外，这段时候Text messaging system也是我们重点关注的内容。（因为和proactive chatbot的交互机制类似，用户都可以接收到信息）</p></li><li><p>用户访谈：主要是对贴吧，B站一些使用角色扮演app的活跃用户进行了访谈，找了5个人，后面也找了几位心理学社会学的同学聊了一下。前者访谈主要是了解目前产品是否有“主动对话”的机制，如果有的话是否存在哪些问题，另一点也是了解他们对“主动对话”的期待；而对于心理学社会学的同学，我们主要是希望去寻找一些“论证主动对话会给用户带来更好体验”的学理依据，有的同学提到了“社会交换”和“意外奖励”理论，也有同学论述了一些场景，让我觉得可以限定对话的一些范围。</p></li><li><p>对话场景的讨论：主要是基于检索到的文献去做了一些对话场景的分类和讨论，然后大家确定了一些对话场景，并做到了大创开题答辩的PPT上。</p></li><li><p>大创答辩：主要是将上述成果写到了大创PPT中，然后去珠海进行了一次申报（3分钟答辩，时间很紧），我们大创答辩做的效果并不是很好，对比了一下其他项目，我发现其他项目重点会说<strong>项目的影响力</strong>和<strong>项目的意义</strong>，例如“弥补国内空白”，“xx产业急需”之类的内容，而我们主要的内容则是介绍了这个Chatbot的工作机制，举了一些对话场景的例子和学理支撑的依据，最后讲了下可能产出就没有了。这个可能也是没有做好的点。</p></li></ul><p>2024年1月-2024年2月，这时我们主要是进入Chatbot的实现阶段，在这个阶段我们一方面和老师还有王博讨论了我们Chatbot的应用场景，最后确定采用”Peer support”+”Stress management”的场景。具体来说，我们希望Chatbot能够扮演用户的一个同伴，然后对用户紧张的事情提供主动的关怀，并且能够分享自己的一些日常生活，以缓解用户压力，提高用户交互兴趣。然后，我们接下来开始具体的实现，在实现过程中，主要参考了以下项目：</p><ul><li><a href="https://github.com/joonspk-research/generative_agents">https://github.com/joonspk-research/generative_agents</a>: 斯坦福AI小镇，这篇paper从立项到结束都看了许多遍，然后这里主要是学习其中prompt的写法（虽然最后结果来看感觉差别还挺大的hhh）</li><li><a href="https://flowgpt.com/">https://flowgpt.com/</a>: 这个平台我们也参考了比较多角色扮演prompt的写法和格式，但因为内容比较零散不太好整合，所以在这里列出来</li><li><a href="https://github.com/Syan-Lin/CyberWaifu">https://github.com/Syan-Lin/CyberWaifu</a>: 这个项目学习了它的记忆系统，这也是目前agent比较通用的记忆系统。</li><li><a href="https://docs.go-cqhttp.org/">https://docs.go-cqhttp.org/</a>: 这个是qq机器人的平台，当时把机器人迁移到了qq上（个人感觉有利有弊，如果自己写前端，写个app的话可能用户体验会更好，因为Chatbot无法识别图片，而且用户在qq聊天会连续发送信息，但好处在于qq很容易交互，也很容易联系用户）</li><li><a href="https://arxiv.org/abs/2106.01144">https://arxiv.org/abs/2106.01144</a>: THU黄组的工作，我们主要参考了其中的策略。黄组和他们的公司（聆心智能）也是国内做LLM role-playing的领先者了，他们后面还推出了一个专门提供心理支持的LLM，可惜当时已经开始user study，不然可能能作为baseline model。</li></ul><p>最后，我们按照这上面的资料加上自己的一些尝试和调整做出了Chatbot系统，并且和几位用户还有合作者尝试后感觉还可以，于是我们就开始了user study。</p><p>2024年3月，我们征集了24个用户进行了为期14天的user study。先说总结，个人感觉user study这一块我们没有做好，主要在于<strong>前期系统设计和统计方法</strong>没有做好，为了弥补这些问题，我做了一个<strong>user study清单（详见末尾）</strong>，用于记录未来user study需要准备和注意的内容。其中，前期系统设计并不是说模块设置的有问题，而是前期在设计的时候，忽略了user study的一些要求。具体来说，user study在加入新用户时需要更新用户名单，然后重启系统。一些设置和变量是储存在文件中的，重启后可以重新载入，但是，有一些变量不是储存在文件中，而是作为临时变量储存在程序中的，所以终止程序后，他们重新被初始化，这就导致了用户体验上出现了些问题（例如Chatbot暂停发送主动对话的时间），虽然在user study第二天后修复了这个问题，整体来看没有特别大的影响，但是在之后设计系统的时候还是需要注意user study的一些考量。</p><p>而对于统计方法，我主要在前期采用的是ANOVA统计方法和独立检验t分布，这是参考两篇user study类似的HHCI paper所选择的方法。然而，在后期统计的时候，我发现一方面ANOVA和独立检验t分布需要满足很多条件（例如ANOVA要求数据正态性），而我们的一些数据<strong>不能满足这个条件</strong>，另一点则是我在后期统计的时候，发现我对于数据的<strong>独立性</strong>理解有误，我将每个参与者每天的打分作为一个独立的样本，但实际上应该是<strong>每个参与者7天&#x2F;14天的平均打分</strong>作为一个独立的样本，因为不同天数相同参与者的得分很难说明是独立的（毕竟一些expectation，experience是很难改变的），这导致了前期一些统计结果在后期直接崩掉了。</p><p>关于统计方法，有一个很好的网站: <a href="https://yatani.jp/teaching/doku.php?id=hcistats:start">https://yatani.jp/teaching/doku.php?id=hcistats:start</a></p><p>我强烈推荐所有做user study的朋友在开始前先把网站的内容浏览一遍，确定<strong>独立样本</strong>、<strong>是否正态</strong>，<strong>是否成对</strong>等数据性质，然后再确定统计方法。统计工具的话，我使用的是spss和python。</p><p>除了统计方法，访谈和交互内容一些文本的定性内容也是我们result的重要组成部分</p><p>在user study中，我觉得心态上要做到四点：一是要提前布局，做好准备，user study不同于AI的实验可以容易复现or重做，14天过一天就是一天，也不能暂停或终止。<strong>所以在user study前，应该预计好每个用户每个时间节点要做的事情</strong>（例如提醒填写每日问卷，做好访谈设计，及时填写PSS-10量表等），同时要确定<strong>我要用什么统计方法，要统计什么数据，要给用户准备什么资料，要用哪些baseline model等</strong>；二是要对自己的系统有信心，在开始user study前自己比较担心用户会有“你做的系统没啥用”的这种观点，但是后面访谈中也<strong>有比较多用户对自己的项目有比较积极的评价</strong>，一方面这些内容充实了user study的定性结果，另一方面收到来自用户的积极评价也是让自己自信心好了些；三是要做好数据的保存和格式重建，user study的数据比较杂也比较多（尤其是后面时间一长，我们24个人，一天就有24份问卷，每张问卷7道题，因此一天的问卷数据量就有100+），到后面很大一部分时间都是在做数据收集的dirty work。而且spss统计分析的一些格式要求比较奇特，所以后面也需要将自己的数据重新整理。对于数据整理，我的建议是<strong>最好放到一个excel文件，采用多个sheet保存不同格式的子表，同时各个sheet规范命名</strong>，如果有多人合作，建议采用腾讯文档去进行分工（不过我觉得最好的模式其实是<strong>一个人专门负责整理数据和进行数据分析</strong>），并且对于数据的格式转换，可以让ChatGPT写一些脚本代码帮忙处理。四是要理性看待结果，我们的research是explore而不是confirm，因此一些出现一些“意料之外”（尤其是没有预计结果好）的结果也很正常，但<strong>可以report这些没有那么“好”的结果</strong>，可以<strong>discuss为什么结果会是这样</strong>。同时，用户对于产品的expectation往往是不一致的，因此提出的一些想法也未必能实现，但同样可以放在future work中论述。</p><h3 id="附录-User-study清单"><a href="#附录-User-study清单" class="headerlink" title="附录: User study清单"></a>附录: User study清单</h3><p>1.RQ:我要在User study研究什么？（系统层面：我要探究哪些模块&#x2F;创新点的作用？Contribution层面：这篇paper的congtributions是什么？）<br>2.Participants: 用户人数是多少？用户要参与多少天？如何筛选用户？用户从哪里找？用户报酬要给多少？用户要分成哪些组？<br>3.Baseline model: 应该选取哪些系统作为Baseline model？Baseline model表现会不会过于强势？<br>4.Measurement &amp; Data analysis: 我要收集统计哪些数据？（包括定量和定性）这些数据对应RQ的哪一部分？这些数据的独立样本是什么？这些数据是否是成对的？如果满足&#x2F;不满足正态性应该采用什么分析方法？这些分析方法在SPSS中要求什么样的格式？<br>5.Task and procedure: 整个user study的流程是什么？（画一个流程图）用户需要在开始前做什么？开始后需要做什么？如果用户有问题应该怎么做？我需要准备什么资料？（介绍，问卷，量表，访谈题目等）<br>6.在user study开始前，我的系统是否能够满足user study的需求？是否需要做一些调整？</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>从Gridea到Halo再到Hexo:记录三次博客的配置之旅</title>
    <link href="/2024/04/09/Post_2024_04_09/"/>
    <url>/2024/04/09/Post_2024_04_09/</url>
    
    <content type="html"><![CDATA[<p>自己想配博客的念头始于2022年10月在看到wk学长的博客后，那时他博客里详细的实验教程以及心路记载让我自己也有了“创建一个博客”的念头，因此在23年2月的时候，自己采用Githubpages也配置了一个hexo博客，然而，这个博客并没有使用多久，就因为我在电脑上用Ubuntu完全覆盖了windows，导致hexo的文件全部丢失而告终；之后，我在Gridea上也创建了一些博客，写了一些简单的玩意：</p><p><a href="https://litijiao.ongridea.com/">https://litijiao.ongridea.com/</a></p><p>但是，也因为各种原因，导致最后没有写太多，加上Gridea的一些内容不是很能定制化，所以自己还是选择了自建站博客。因此，在2024年2月，我创建了一个Halo的博客，Halo的好处是动态的，可以直接在浏览器上写（让我回想起了以前QQ空间的日志2333），而且Halo的配置简单，主题多样，因此我觉得Halo是个不错的选择。</p><p>然而，在3月，因为自己专注大创项目，导致忘记续费云主机，最后导致Halo博客被关闭。因此，在我重建Blog的时候，我也在思考，我需要一个怎样的博客？</p><ul><li><p>这个博客最好能免费，因为我不确定自己写博客的热情能坚持多少，如果是云主机最便宜的一个月也要30，这个也是蛮大的一笔开销。</p></li><li><p>这个博客最好配置不要太麻烦，主要是我对于前端的工作其实并不是很了解，而且一些网络的配置自己也不是很明白，自己最主要是想找个可以记录自己“质量不高”文字的地方（质量高的可能我更愿意发知乎等社交平台）。</p></li></ul><p>所以，最后我还是选择回hexo，借助githubpage和自己已经有的域名，可以免费做一个还可以的网站，这些教程也给了我重要的参考：</p><p><a href="https://zhuanlan.zhihu.com/p/60578464">https://zhuanlan.zhihu.com/p/60578464</a></p><p><a href="https://fluid-dev.github.io/hexo-fluid-docs/start/">https://fluid-dev.github.io/hexo-fluid-docs/start/</a></p><p><a href="https://blog.ayaka.space/2024/01/From-Halo-To-Hexo/">https://blog.ayaka.space/2024/01/From-Halo-To-Hexo/</a></p><p>总之，希望这次自己不要半途而废，能够在这里多记录一些所学所想。</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2024/04/09/hello-world/"/>
    <url>/2024/04/09/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
